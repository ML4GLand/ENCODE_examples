{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/opt/miniconda3/envs/enformer-pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from enformer_pytorch import from_pretrained\n",
    "from enformer_pytorch.finetune import HeadAdapterWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enformer = from_pretrained('EleutherAI/enformer-official-rough', use_tf_gamma = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadAdapterWrapper(\n",
       "  (enformer): Enformer(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_tower): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (crop_final): TargetLengthCrop()\n",
       "    (final_pointwise): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Rearrange('b d n -> b n d')\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): GELU()\n",
       "    )\n",
       "    (_trunk): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Rearrange('b d n -> b n d')\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TargetLengthCrop()\n",
       "      (6): Sequential(\n",
       "        (0): Rearrange('b n d -> b d n')\n",
       "        (1): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Rearrange('b d n -> b n d')\n",
       "        (3): Dropout(p=0.05, inplace=False)\n",
       "        (4): GELU()\n",
       "      )\n",
       "    )\n",
       "    (_heads): ModuleDict(\n",
       "      (human): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=5313, bias=True)\n",
       "        (1): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mouse): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=1643, bias=True)\n",
       "        (1): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_embed_transform): Sequential(\n",
       "    (0): Identity()\n",
       "  )\n",
       "  (to_tracks): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=128, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HeadAdapterWrapper(\n",
    "    enformer = enformer,\n",
    "    num_tracks = 128,\n",
    "    post_transformer_embed = False   # by default, embeddings are taken from after the final pointwise block w/ conv -> gelu - but if you'd like the embeddings right after the transformer block with a learned layernorm, set this to True\n",
    ").cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = torch.randint(0, 5, (1, 196_608 // 2,)).cuda()\n",
    "target = torch.randn(1, 200, 128).cuda()  # 128 tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(seq, target = target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6934, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 enformer-pytorch",
   "language": "python",
   "name": "enformer-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
