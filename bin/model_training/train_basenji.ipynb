{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(1.702 * x) * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exponential(nn.Module):\n",
    "    \"\"\"Custom exponential activation\"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(activation):\n",
    "    activation = activation.lower()\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'gelu':\n",
    "        return nn.GELU()\n",
    "    elif activation == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation == 'exponential':  # Custom implementation\n",
    "        return Exponential()\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus()\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized activation \"{activation}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your input tensor\n",
    "input_tensor = torch.randn((1, 3, 224, 224))\n",
    "\n",
    "# Get the activation layer\n",
    "activation_layer = activate('gelu')  # replace 'gelu' with the activation you want\n",
    "\n",
    "# Apply the activation to your input tensor\n",
    "output = activation_layer(input_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, kernel_size, padding='same', activation='relu', dropout=0.2):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        # Calculate padding for 'same'\n",
    "        if padding == 'same':\n",
    "            padding = kernel_size // 2\n",
    "\n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=num_filters, kernel_size=kernel_size,\n",
    "                              stride=1, padding=padding, bias=False)\n",
    "\n",
    "        # Batch normalization\n",
    "        self.bn = nn.BatchNorm1d(num_features=num_filters)\n",
    "\n",
    "        # Activation\n",
    "        self.activation = activate(activation)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your input tensor\n",
    "input_tensor = torch.randn((10, 4, 2048))\n",
    "\n",
    "# Create an instance of the ConvLayer\n",
    "conv_layer = ConvLayer(4, 256, 15, padding='same', activation='relu', dropout=0.2)\n",
    "\n",
    "# Apply the conv_layer to your input tensor\n",
    "output = conv_layer(input_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        filters=None, \n",
    "        kernel_size=1, \n",
    "        activation='relu', \n",
    "        activation_end=None,\n",
    "        strides=1, \n",
    "        dilation_rate=1, \n",
    "        l2_scale=0, \n",
    "        dropout=0, \n",
    "        conv_type='standard', \n",
    "        residual=False,\n",
    "        pool_size=1, \n",
    "        batch_norm=False, \n",
    "        bn_momentum=0.99, \n",
    "        bn_gamma=None, \n",
    "        padding='same', \n",
    "        w1=False\n",
    "    ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.pool_size = pool_size\n",
    "        self.padding = padding if padding != 'same' else kernel_size // 2\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        if filters is None:\n",
    "            filters = in_channels\n",
    "\n",
    "        # Choose convolution type\n",
    "        if conv_type == 'separable':\n",
    "            # PyTorch doesn't have a direct equivalent of Keras's SeparableConv1D.\n",
    "            raise NotImplementedError(\"SeparableConv1D is not directly implemented in PyTorch.\")\n",
    "        elif w1:\n",
    "            self.conv = nn.Conv2d(in_channels, filters, kernel_size, stride=strides, padding=self.padding, dilation=dilation_rate, bias=False)\n",
    "        else:\n",
    "            self.conv = nn.Conv1d(in_channels, filters, kernel_size, stride=strides, padding=self.padding, dilation=dilation_rate, bias=False)\n",
    "\n",
    "        # Batch normalization\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(num_features=filters, momentum=bn_momentum)  # Adjust for 2D if using Conv2d\n",
    "\n",
    "        # Activation\n",
    "        self.activation = activate(activation)  # Use the activate function from previous example\n",
    "        self.activation_end = activate(activation_end) if activation_end else None\n",
    "\n",
    "        # Dropout\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        # Pooling\n",
    "        if pool_size > 1:\n",
    "            if w1:\n",
    "                self.pool = nn.MaxPool2d(kernel_size=pool_size, padding=self.padding)\n",
    "            else:\n",
    "                self.pool = nn.MaxPool1d(kernel_size=pool_size, padding=self.padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x if self.residual else None\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        x = self.activation(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if self.residual:\n",
    "            x = x + residual\n",
    "\n",
    "        if self.activation_end:\n",
    "            x = self.activation_end(x)\n",
    "\n",
    "        if self.pool_size > 1:\n",
    "            x = self.pool(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your input tensor\n",
    "input_tensor = torch.randn((10, 4, 2048))\n",
    "\n",
    "# Create an instance of the ConvBlock\n",
    "conv_block = ConvBlock(4, filters=256, kernel_size=15, padding='same', activation='relu', dropout=0.2)\n",
    "\n",
    "# Apply the conv_block to your input tensor\n",
    "output = conv_block(input_tensor)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ConvTower(nn.Module):\n",
    "    def __init__(self, in_channels, filters_init, filters_mult=1, repeat=1, **kwargs):\n",
    "        super(ConvTower, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        rep_filters = filters_init\n",
    "\n",
    "        for ri in range(repeat):\n",
    "            # Round filters to the nearest integer for the current block\n",
    "            current_filters = int(round(rep_filters))\n",
    "            # Create a new convolution block with the specified number of filters\n",
    "            conv_block_layer = ConvBlock(in_channels=in_channels if ri == 0 else current_filters,\n",
    "                                         filters=current_filters,\n",
    "                                         **kwargs)\n",
    "            self.layers.append(conv_block_layer)\n",
    "            # Update in_channels for the next block and rep_filters for the entire sequence\n",
    "            in_channels = current_filters\n",
    "            rep_filters *= filters_mult\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your input tensor\n",
    "input_tensor = torch.randn((10, 4, 2048))\n",
    "\n",
    "# Create an instance of the ConvTower\n",
    "conv_tower = ConvTower(in_channels=4, filters_init=64, filters_mult=1, repeat=3)\n",
    "\n",
    "# Apply the conv_tower to your input tensor\n",
    "output = conv_tower(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTower(\n",
       "  (layers): ModuleList(\n",
       "    (0): ConvBlock(\n",
       "      (conv): Conv1d(4, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1-2): 2 x ConvBlock(\n",
       "      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 2048])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 ml4gland",
   "language": "python",
   "name": "ml4gland"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
